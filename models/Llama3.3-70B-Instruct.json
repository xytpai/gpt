{
  "dim": 8192,
  "n_layers": 80,
  "n_heads": 64,
  "n_kv_heads": 8,
  "vocab_size": 128256,
  "ffn_dim_multiplier": 1.3,
  "multiple_of": 4096,
  "norm_eps": 1e-05,
  "rope_theta": 500000.0,
  "use_scaled_rope": true,
  "dtype": "torch.half",

  "base_dir": "/home/xytpai/.llama/checkpoints/Llama3.3-70B-Instruct",
  "wfiles": [
    "consolidated.00.pth",
    "consolidated.01.pth",
    "consolidated.02.pth",
    "consolidated.03.pth",
    "consolidated.04.pth",
    "consolidated.05.pth",
    "consolidated.06.pth",
    "consolidated.07.pth"],
  "tfile": "tokenizer.model"
}
